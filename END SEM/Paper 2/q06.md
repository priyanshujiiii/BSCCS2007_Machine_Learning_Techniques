![image](https://user-images.githubusercontent.com/89120960/234552418-4df6c5f7-ed47-4847-a13b-4f53bfd45b2a.png)



Sure! Here is the formatted solution:

The correct expression for the negative log-likelihood (binary cross entropy loss) of the logistic regression model on the given dataset is:

∑
i=1
n
−log⁡p_i + ∑
i=n+1
2n
−log⁡(1−p_i)

This expression consists of two summations. The first summation goes over the n data-points belonging to class-1, and for each data-point, it computes the negative log of the predicted probability of class-1 (p_i) using the log function. The second summation goes over the n data-points belonging to class-0, and for each data-point, it computes the negative log of the predicted probability of class-0 (1 - p_i) using the log function. The factor of 1/2 in front of the second summation is because class-0 has 2n data-points.
