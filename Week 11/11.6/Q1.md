![image](https://user-images.githubusercontent.com/89120960/232326388-6440ec8b-0c7d-4ff6-8534-db183703a611.png)


<p>
  False. 

In AdaBoost, estimators can be trained parallelly. Each estimator in the sequence is fitted on a modified version of the original dataset that weights each sample by its misclassification rate from the previous estimator. Therefore, each estimator can be trained independently of the others, allowing for parallel computation.
</p>
