![image](https://user-images.githubusercontent.com/89120960/232327751-26ed3696-edbc-4bd7-ae7f-87b14957c5e9.png)



<p>
  The correct expression is:

D[t+1](i) = D[t](i) * exp(-alpha[t]) if h[t](x[i]) = y[i]

D[t+1](i) = D[t](i) * exp(alpha[t]) if h[t](x[i]) != y[i]

where alpha[t] = 0.5 * ln((1-error[t]) / error[t]) is the weight of the t-th weak learner, error[t] is the weighted error of the t-th weak learner, h[t](x[i]) is the prediction of the t-th weak learner for the i-th data point, y[i] is the true label of the i-th data point, and D[t](i) is the weight of the i-th data point at round t. 

Note that the weight of the i-th data point at round t+1 depends on its weight at round t and the performance of the t-th weak learner on the i-th data point. If the t-th weak learner correctly classifies the i-th data point, its weight is decreased by a factor of exp(-alpha[t]). Otherwise, its weight is increased by a factor of exp(alpha[t]). This way, AdaBoost gives more weight to the misclassified data points, which makes the subsequent weak learners focus more on these points, leading to better overall performance.
</p>
