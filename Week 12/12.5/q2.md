![image](https://user-images.githubusercontent.com/89120960/232330231-4953e3f6-9a1a-48b5-901f-647ef1f13efc.png)


<p>
  The statement is False.

Back-propagation is a widely used technique for training neural networks. It is an iterative algorithm that adjusts the weights of the model to minimize the loss function. However, it does not guarantee that it will find the global minimum of the loss function. In fact, the loss function may have many local minima, and back-propagation may converge to one of them instead of the global minimum. 

Therefore, the success of back-propagation in finding a good set of model parameters depends on the initial conditions and the choice of optimization algorithm used. Various techniques such as early stopping, regularization, and momentum are used to improve the chances of finding a good set of parameters.
</p>
