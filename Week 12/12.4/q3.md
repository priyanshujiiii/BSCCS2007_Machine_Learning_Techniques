![image](https://user-images.githubusercontent.com/89120960/232330068-5be356a8-80e9-4dca-8207-742e0205cc02.png)



<p>
  0-1 function may not be an appropriate choice for the activation function to be used for the hidden layers of a neural network, as it is not continuous and not differentiable, which makes it unsuitable for backpropagation algorithm. The sigmoid function and ReLU function are popular choices for activation functions in neural networks.
</p>
