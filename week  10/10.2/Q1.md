![image](https://user-images.githubusercontent.com/89120960/232202857-4b149571-bb54-42ed-a4a7-36e0611bf370.png)


<p>
  The constraints for a linearly separable dataset with margin γ are:

For each data point (x
i
, y
i
), where x
i
is the feature vector and y
i
is the label:

y
i
(w
T
x
i
+b) ≥ γ if y
i
= 1
y
i
(w
T
x
i
+b) ≤ -γ if y
i
= -1
Here, w is the weight vector, b is the bias term, and T denotes the transpose.

These constraints ensure that the distance between the hyperplane and the closest data points on either side is at least γ, which in turn ensures that the margin is at least γ. The optimization problem for finding the maximum-margin hyperplane can then be formulated as a constrained optimization problem that maximizes γ subject to these constraints. This is the basic idea behind the support vector machine (SVM) algorithm for training linear classifiers.
</p>
